{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Let's define our configuration class first - it's like the blueprint for our BERT model\n",
        "class BertConfig:\n",
        "    def __init__(self,\n",
        "                 vocab_size=30522,         # How many words we know\n",
        "                 hidden_size=768,          # Size of our hidden layers\n",
        "                 num_hidden_layers=12,     # How many transformer layers to stack\n",
        "                 num_attention_heads=12,   # Number of attention heads\n",
        "                 intermediate_size=3072,   # Size of the feed-forward network\n",
        "                 hidden_dropout_prob=0.1,  # Dropout probability\n",
        "                 attention_probs_dropout_prob=0.1,\n",
        "                 max_position_embeddings=512,\n",
        "                 type_vocab_size=2):       # Usually 2 for sentence A/B\n",
        "        # Just storing all our config values\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "\n",
        "# This handles turning our input tokens into proper embeddings\n",
        "class BertEmbeddings(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Three types of embeddings we'll combine\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # Normalization and dropout for stability\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        # If we don't get token types, assume they're all zeros\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        # Generate position IDs for each token in sequence\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "        # Get all our embeddings and add them together\n",
        "        words = self.word_embeddings(input_ids)\n",
        "        positions = self.position_embeddings(position_ids)\n",
        "        token_types = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words + positions + token_types\n",
        "\n",
        "        # Normalize and apply dropout\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "# The heart of the attention mechanism\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Make sure hidden size works with number of heads\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\"Hey, the hidden size needs to be divisible by number of attention heads!\")\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
        "        self.all_head_size = config.hidden_size\n",
        "\n",
        "        # Linear layers for Q, K, V\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        # Reshape for multi-head attention\n",
        "        new_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        # Calculate Q, K, V and reshape them\n",
        "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
        "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        # Attention scores - the magic happens here!\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "\n",
        "        # Apply mask if we have one\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Convert scores to probabilities\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Get our final output\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        context_layer = context_layer.view(*context_layer.size()[:-2], self.all_head_size)\n",
        "\n",
        "        return context_layer\n",
        "\n",
        "# Putting it all together for a complete attention block\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        # Run attention and process the output\n",
        "        self_output = self.self(hidden_states, attention_mask)\n",
        "        proj_output = self.output(self_output)\n",
        "        proj_output = self.dropout(proj_output)\n",
        "        # Add residual connection and normalize\n",
        "        return self.LayerNorm(hidden_states + proj_output)\n",
        "\n",
        "# The feed-forward part of the transformer\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = nn.GELU()  # GELU seems to work better than ReLU here\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        return self.intermediate_act_fn(self.dense(hidden_states))\n",
        "\n",
        "# Final output layer with residual connection\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        return self.LayerNorm(hidden_states + input_tensor)\n",
        "\n",
        "# A complete transformer layer\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        # Attention first, then feed-forward\n",
        "        attn_output = self.attention(hidden_states, attention_mask)\n",
        "        intermediate = self.intermediate(attn_output)\n",
        "        return self.output(intermediate, attn_output)\n",
        "\n",
        "# Stack up all our layers\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        # Run through each layer\n",
        "        for layer in self.layer:\n",
        "            hidden_states = layer(hidden_states, attention_mask)\n",
        "        return hidden_states\n",
        "\n",
        "# The full BERT model!\n",
        "class BertModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
        "        # Create attention mask if none provided\n",
        "        if attention_mask is None:\n",
        "            attention_mask = (input_ids != 0).unsqueeze(1).unsqueeze(2).float()\n",
        "            attention_mask = (1.0 - attention_mask) * -10000.0\n",
        "\n",
        "        # Get embeddings and run through encoder\n",
        "        embeddings = self.embeddings(input_ids, token_type_ids)\n",
        "        return self.encoder(embeddings, attention_mask)\n",
        "\n",
        "# Let's test it out\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a small version for testing\n",
        "    config = BertConfig(\n",
        "        vocab_size=30522,\n",
        "        hidden_size=128,      # Smaller than default for testing\n",
        "        num_hidden_layers=2,  # Just 2 layers to keep it simple\n",
        "        num_attention_heads=2,\n",
        "        intermediate_size=512\n",
        "    )\n",
        "\n",
        "    model = BertModel(config)\n",
        "\n",
        "    # Some dummy input data\n",
        "    input_ids = torch.tensor([\n",
        "        [101, 7592, 2023, 102, 0, 0],    # \"Hello this\" + padding\n",
        "        [101, 2023, 2003, 1037, 3978, 102] # \"This is a test\"\n",
        "    ])\n",
        "    token_type_ids = torch.tensor([\n",
        "        [0, 0, 0, 0, 0, 0],\n",
        "        [0, 0, 0, 0, 1, 1]\n",
        "    ])\n",
        "\n",
        "    output = model(input_ids, token_type_ids)\n",
        "    print(f\"Got output shape: {output.shape}\")  # Should be [2, 6, 128]"
      ],
      "metadata": {
        "id": "z129ubqbmNdk",
        "outputId": "2f1b1b4f-ad4a-4170-d065-35c7930360c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got output shape: torch.Size([2, 6, 128])\n"
          ]
        }
      ]
    }
  ]
}