{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeyoDjqoEeO5",
        "outputId": "267dd82d-4747-4a76-8f3a-601d28a14fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-31 16:02:03--  https://pjreddie.com/media/files/mnist_train.csv\n",
            "Resolving pjreddie.com (pjreddie.com)... 162.0.215.52\n",
            "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 109575994 (104M) [text/csv]\n",
            "Saving to: ‘mnist_train.csv.1’\n",
            "\n",
            "mnist_train.csv.1   100%[===================>] 104.50M  14.2MB/s    in 8.3s    \n",
            "\n",
            "2025-01-31 16:02:13 (12.6 MB/s) - ‘mnist_train.csv.1’ saved [109575994/109575994]\n",
            "\n",
            "--2025-01-31 16:02:13--  https://pjreddie.com/media/files/mnist_test.csv\n",
            "Resolving pjreddie.com (pjreddie.com)... 162.0.215.52\n",
            "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18289443 (17M) [text/csv]\n",
            "Saving to: ‘mnist_test.csv.1’\n",
            "\n",
            "mnist_test.csv.1    100%[===================>]  17.44M  8.30MB/s    in 2.1s    \n",
            "\n",
            "2025-01-31 16:02:16 (8.30 MB/s) - ‘mnist_test.csv.1’ saved [18289443/18289443]\n",
            "\n",
            "training data shape: (59999, 785)\n",
            "test data shape: (9999, 785)\n",
            "<bound method NDFrame.head of        5  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  ...  0.608  0.609  0.610  \\\n",
            "0      0  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
            "1      4  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
            "2      1  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
            "3      9  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
            "4      2  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
            "...   .. ..  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...    ...   \n",
            "59994  8  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
            "59995  3  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
            "59996  5  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
            "59997  6  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
            "59998  8  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
            "\n",
            "       0.611  0.612  0.613  0.614  0.615  0.616  0.617  \n",
            "0          0      0      0      0      0      0      0  \n",
            "1          0      0      0      0      0      0      0  \n",
            "2          0      0      0      0      0      0      0  \n",
            "3          0      0      0      0      0      0      0  \n",
            "4          0      0      0      0      0      0      0  \n",
            "...      ...    ...    ...    ...    ...    ...    ...  \n",
            "59994      0      0      0      0      0      0      0  \n",
            "59995      0      0      0      0      0      0      0  \n",
            "59996      0      0      0      0      0      0      0  \n",
            "59997      0      0      0      0      0      0      0  \n",
            "59998      0      0      0      0      0      0      0  \n",
            "\n",
            "[59999 rows x 785 columns]>\n",
            "X_train shape: (59999, 784)\n",
            "Y_train shape: (59999,)\n",
            "X_test shape: (9999, 784)\n",
            "Y_test shape: (9999,)\n",
            "minimum value in X_train: 0.0\n",
            "maximum value in X_train 1.0\n",
            "Y_train_encoded shape: (59999, 10)\n",
            "Example of one-hot encoded label: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "W1 shape: (784, 128)\n",
            "b1 shape: (1, 128)\n",
            "W2 shape: (128, 10)\n",
            "b2 shape: (1, 10)\n",
            "Iteration 0, Loss: 2.30309053475388\n",
            "Iteration 100, Loss: 0.9512096041953741\n",
            "Iteration 200, Loss: 0.504467385266919\n",
            "Iteration 300, Loss: 0.4050410225065792\n",
            "Iteration 400, Loss: 0.3615587381556752\n",
            "Iteration 500, Loss: 0.3354798590860831\n",
            "Iteration 600, Loss: 0.31674208759501776\n",
            "Iteration 700, Loss: 0.3018320746207779\n",
            "Iteration 800, Loss: 0.28918717773297203\n",
            "Iteration 900, Loss: 0.2779080147182539\n"
          ]
        }
      ],
      "source": [
        "### IMPLEMENTATION OF A DIGIT CLASSIFIER OF MNIST DATASET USING NUMPY\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!wget https://pjreddie.com/media/files/mnist_train.csv\n",
        "!wget https://pjreddie.com/media/files/mnist_test.csv\n",
        "\n",
        "\n",
        "train_data=pd.read_csv(\"mnist_train.csv\")\n",
        "test_data=pd.read_csv(\"mnist_test.csv\")\n",
        "\n",
        "print(\"training data shape:\", train_data.shape)\n",
        "print(\"test data shape:\", test_data.shape)\n",
        "\n",
        "# let me print a few rows to see what the data looks like\n",
        "\n",
        "print(train_data.head)\n",
        "\n",
        "X_train = train_data.iloc[:,1:].values\n",
        "Y_train = train_data.iloc[:,0].values\n",
        "\n",
        "X_test = test_data.iloc[:,1:].values\n",
        "Y_test = test_data.iloc[:,0].values\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"Y_train shape:\", Y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"Y_test shape:\", Y_test.shape)\n",
        "\n",
        "#next lets normalise the values :D\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "print(\"minimum value in X_train:\", np.min(X_train))\n",
        "print(\"maximum value in X_train\", np.max(X_train))\n",
        "\n",
        "#next we will perform one hot encoding :D\n",
        "\n",
        "def oneh_encode(Y, num_classes=10):\n",
        "    one_hot = np.zeros((Y.size, num_classes))\n",
        "    one_hot[np.arange(Y.size), Y] = 1\n",
        "    return one_hot\n",
        "\n",
        "Y_train_encoded = oneh_encode(Y_train)\n",
        "Y_test_encoded = oneh_encode(Y_test)\n",
        "\n",
        "print(\"Y_train_encoded shape:\", Y_train_encoded.shape)\n",
        "print(\"Example of one-hot encoded label:\", Y_train_encoded[0])\n",
        "\n",
        "# Set the size of each layer\n",
        "input_size = 784\n",
        "hidden_size = 128\n",
        "output_size = 10\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "\n",
        "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"W1 shape:\", W1.shape)\n",
        "print(\"b1 shape:\", b1.shape)\n",
        "print(\"W2 shape:\", W2.shape)\n",
        "print(\"b2 shape:\", b2.shape)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "    # Hidden layer\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)  # Apply ReLU activation\n",
        "\n",
        "    # Output layer\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = softmax(Z2)  # Apply softmax activation\n",
        "\n",
        "    # Store activations for backpropagation\n",
        "    op = (Z1, A1, Z2, A2)\n",
        "    return A2, op\n",
        "\n",
        "# we are going to use cross entropy loss :D\n",
        "\n",
        "def compute_loss(Y,A2):\n",
        "    m = Y.shape[0]\n",
        "    loss = -np.sum(Y*np.log(A2 + 1e-8)) / m\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)  # Derivative of ReLU: 1 if x > 0, else 0\n",
        "\n",
        "def backward_propagation(X, Y, cache, W1, W2):\n",
        "    Z1, A1, Z2, A2 = cache\n",
        "    m = X.shape[0]  # Number of examples\n",
        "\n",
        "    # Output layer error\n",
        "    dZ2 = A2 - Y  # Error at output\n",
        "    dW2 = np.dot(A1.T, dZ2) / m  # Gradient of W2\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m  # Gradient of b2\n",
        "\n",
        "    # Hidden layer error\n",
        "    dA1 = np.dot(dZ2, W2.T)  # Backpropagate through W2\n",
        "    dZ1 = dA1 * relu_derivative(Z1)  # Element-wise multiplication\n",
        "    dW1 = np.dot(X.T, dZ1) / m  # Gradient of W1\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m  # Gradient of b1\n",
        "\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
        "    # Update weights and biases using gradient descent\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "\n",
        "#training loop\n",
        "def train(X, Y, W1, b1, W2, b2, iterations, learning_rate):\n",
        "    for i in range(iterations):\n",
        "        # Step 1: forward propagation\n",
        "        A2, op = forward_propagation(X, W1, b1, W2, b2)\n",
        "\n",
        "        # Step 2: compute the loss\n",
        "        loss = compute_loss(Y, A2)\n",
        "\n",
        "        # Print the loss every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}, Loss: {loss}\")\n",
        "\n",
        "\n",
        "        # Step 3: Backpropagation\n",
        "        dW1, db1, dW2, db2 = backward_propagation(X, Y, op, W1, W2)\n",
        "\n",
        "        # Step 4: Update parameters\n",
        "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "\n",
        "iterations = 1000\n",
        "learning_rate = 0.1\n",
        "\n",
        "#training the neural network\n",
        "W1, b1, W2, b2 = train(X_train, Y_train_encoded, W1, b1, W2, b2, iterations, learning_rate)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, W1, b1, W2, b2):\n",
        "    A2, _ = forward_propagation(X, W1, b1, W2, b2)\n",
        "    predictions = np.argmax(A2, axis=1)\n",
        "    return predictions\n",
        "\n",
        "# Making predictions on the test set\n",
        "predictions = predict(X_test, W1, b1, W2, b2)\n",
        "accuracy = np.mean(predictions == Y_test) * 100\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4i4PvBIK-hz",
        "outputId": "c5c8f9ed-4b0a-4cb0-9cc5-04923542f191"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 92.72%\n"
          ]
        }
      ]
    }
  ]
}