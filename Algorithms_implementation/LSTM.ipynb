{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZcczH77Jby3",
        "outputId": "c556f21d-d538-4a13-d68f-0af029cc0353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[-0.0052],\n",
            "        [-0.0244],\n",
            "        [-0.0117],\n",
            "        [ 0.0190],\n",
            "        [-0.0176]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTMmodel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMmodel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "\n",
        "        last_time_step = out[:, -1, :]\n",
        "\n",
        "        out = self.fc(last_time_step)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters for our dummy data and model\n",
        "input_size = 10    # Number of features per time step in the input\n",
        "hidden_size = 20   # Number of features in the hidden state of the LSTM\n",
        "num_layers = 2     # Number of stacked LSTM layers\n",
        "output_size = 1    # Dimension of the model's output\n",
        "batch_size = 5     # Number of sequences in a batch\n",
        "seq_length = 7     # Number of time steps in each sequence\n",
        "\n",
        "model = LSTMmodel(input_size, hidden_size, num_layers, output_size)\n",
        "\n",
        "x = torch.randn(batch_size, seq_length, input_size) #dummy\n",
        "\n",
        "output = model(x)\n",
        "\n",
        "print(\"Output:\", output)\n"
      ]
    }
  ]
}