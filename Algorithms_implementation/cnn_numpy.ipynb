{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6pLlyLx6RUo",
        "outputId": "c1318085-8c29-4920-fa35-888040798798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-7e8fdb05ae38>:36: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  out[n, f, i, j] = np.sum(X_padded[n, :, h_start:h_end, w_start:w_end] * self.W[f]) + self.b[f]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, CNN loss: 2.9075\n",
            "Iteration 10, CNN loss: 1.8861\n",
            "Iteration 20, CNN loss: 1.2991\n",
            "Iteration 30, CNN loss: 1.0878\n",
            "Iteration 40, CNN loss: 0.4380\n",
            "Iteration 50, CNN loss: 0.3618\n",
            "Iteration 60, CNN loss: 0.2631\n",
            "Iteration 70, CNN loss: 0.0980\n",
            "Iteration 80, CNN loss: 0.0748\n",
            "Iteration 90, CNN loss: 0.1113\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Convolutional Layer\n",
        "class ConvLayer:\n",
        "    def __init__(self, in_channels, out_channels, filter_size, stride=1, padding=0):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.filter_size = filter_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        # Initialize weights and biases\n",
        "        self.W = np.random.randn(out_channels, in_channels, filter_size, filter_size) * 0.1\n",
        "        self.b = np.zeros((out_channels, 1))\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        self.X = X  # Cache for backward pass\n",
        "        N, C, H, W = X.shape\n",
        "        # Calculate output dimensions\n",
        "        H_out = int((H - self.filter_size + 2*self.padding) / self.stride + 1)\n",
        "        W_out = int((W - self.filter_size + 2*self.padding) / self.stride + 1)\n",
        "        # Pad input if necessary\n",
        "        X_padded = np.pad(X, ((0,0), (0,0), (self.padding,self.padding), (self.padding,self.padding)), mode='constant')\n",
        "        self.X_padded = X_padded  # Cache for backward\n",
        "        out = np.zeros((N, self.out_channels, H_out, W_out))\n",
        "\n",
        "        # Naively slide the filter over the input\n",
        "        for n in range(N):\n",
        "            for f in range(self.out_channels):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        h_start = i * self.stride\n",
        "                        h_end = h_start + self.filter_size\n",
        "                        w_start = j * self.stride\n",
        "                        w_end = w_start + self.filter_size\n",
        "                        out[n, f, i, j] = np.sum(X_padded[n, :, h_start:h_end, w_start:w_end] * self.W[f]) + self.b[f]\n",
        "        return out\n",
        "\n",
        "    def backward(self, d_out, learning_rate):\n",
        "        \"\"\"\n",
        "        d_out: Gradient of loss with respect to output, shape (N, out_channels, H_out, W_out)\n",
        "        Updates the layer's parameters using gradient descent.\n",
        "        Returns:\n",
        "            dX: Gradient with respect to input X.\n",
        "        \"\"\"\n",
        "        X = self.X\n",
        "        X_padded = self.X_padded\n",
        "        N, C, H, W = X.shape\n",
        "        _, _, H_out, W_out = d_out.shape\n",
        "\n",
        "        dW = np.zeros_like(self.W)\n",
        "        db = np.zeros_like(self.b)\n",
        "        dX_padded = np.zeros_like(X_padded)\n",
        "\n",
        "        for n in range(N):\n",
        "            for f in range(self.out_channels):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        h_start = i * self.stride\n",
        "                        h_end = h_start + self.filter_size\n",
        "                        w_start = j * self.stride\n",
        "                        w_end = w_start + self.filter_size\n",
        "                        # Gradient with respect to weights\n",
        "                        dW[f] += d_out[n, f, i, j] * X_padded[n, :, h_start:h_end, w_start:w_end]\n",
        "                        # Gradient with respect to bias\n",
        "                        db[f] += d_out[n, f, i, j]\n",
        "                        # Gradient with respect to the input\n",
        "                        dX_padded[n, :, h_start:h_end, w_start:w_end] += d_out[n, f, i, j] * self.W[f]\n",
        "\n",
        "        # Remove padding from gradient if necessary\n",
        "        if self.padding > 0:\n",
        "            dX = dX_padded[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
        "        else:\n",
        "            dX = dX_padded\n",
        "\n",
        "        # Update parameters\n",
        "        self.W -= learning_rate * dW\n",
        "        self.b -= learning_rate * db\n",
        "\n",
        "        return dX\n",
        "\n",
        "# ReLU Activation\n",
        "class ReLU:\n",
        "    def forward(self, X):\n",
        "        self.X = X  # Cache for backward pass\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        dX = d_out.copy()\n",
        "        dX[self.X <= 0] = 0\n",
        "        return dX\n",
        "\n",
        "# Fully Connected (Dense) Layer\n",
        "class FullyConnected:\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.W = np.random.randn(input_dim, output_dim) * 0.1\n",
        "        self.b = np.zeros((1, output_dim))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X  # Cache for backward\n",
        "        return X.dot(self.W) + self.b\n",
        "\n",
        "    def backward(self, d_out, learning_rate):\n",
        "        dW = self.X.T.dot(d_out)\n",
        "        db = np.sum(d_out, axis=0, keepdims=True)\n",
        "        dX = d_out.dot(self.W.T)\n",
        "\n",
        "        # Update parameters\n",
        "        self.W -= learning_rate * dW\n",
        "        self.b -= learning_rate * db\n",
        "\n",
        "        return dX\n",
        "\n",
        "# Softmax Loss\n",
        "def softmax_loss(scores, y):\n",
        "    shifted_scores = scores - np.max(scores, axis=1, keepdims=True)  # For numerical stability\n",
        "    exp_scores = np.exp(shifted_scores)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "    N = scores.shape[0]\n",
        "    loss = -np.sum(np.log(probs[np.arange(N), y])) / N\n",
        "\n",
        "    dscores = probs.copy()\n",
        "    dscores[np.arange(N), y] -= 1\n",
        "    dscores /= N\n",
        "\n",
        "    return loss, dscores\n",
        "\n",
        "# Simple CNN Model\n",
        "class SimpleCNN:\n",
        "    def __init__(self, num_classes=10):\n",
        "        # For simplicity, we assume input images are 28x28 with 1 channel (e.g., MNIST)\n",
        "        # Our conv layer uses padding=1 to preserve spatial dimensions.\n",
        "        self.conv = ConvLayer(in_channels=1, out_channels=2, filter_size=3, stride=1, padding=1)\n",
        "        self.relu = ReLU()\n",
        "        # After convolution, the output shape is (N, 2, 28, 28).\n",
        "        # We flatten it to a vector of size 2*28*28.\n",
        "        self.fc = FullyConnected(input_dim=2*28*28, output_dim=num_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = self.conv.forward(X)\n",
        "        out = self.relu.forward(out)\n",
        "        self.shape_cache = out.shape  # Cache shape for backward\n",
        "        out_flat = out.reshape(out.shape[0], -1)\n",
        "        scores = self.fc.forward(out_flat)\n",
        "        return scores\n",
        "\n",
        "    def backward(self, dscores, learning_rate):\n",
        "        d_out_flat = self.fc.backward(dscores, learning_rate)\n",
        "        d_out = d_out_flat.reshape(self.shape_cache)\n",
        "        d_out = self.relu.backward(d_out)\n",
        "        self.conv.backward(d_out, learning_rate)\n",
        "\n",
        "#Training Loop for the CNN\n",
        "def train_cnn():\n",
        "    np.random.seed(0)\n",
        "    cnn = SimpleCNN(num_classes=10)\n",
        "    learning_rate = 0.01\n",
        "    num_iters = 100\n",
        "    batch_size = 5\n",
        "\n",
        "    # Create dummy data: 20 random grayscale images of shape 28x28 and random labels (0 to 9)\n",
        "    X = np.random.randn(20, 1, 28, 28)\n",
        "    y = np.random.randint(0, 10, size=20)\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        # Select a random mini-batch\n",
        "        idx = np.random.choice(20, batch_size, replace=False)\n",
        "        X_batch = X[idx]\n",
        "        y_batch = y[idx]\n",
        "\n",
        "        # Forward pass\n",
        "        scores = cnn.forward(X_batch)\n",
        "        loss, dscores = softmax_loss(scores, y_batch)\n",
        "\n",
        "        # Backward pass and parameter update\n",
        "        cnn.backward(dscores, learning_rate)\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Iteration {i}, CNN loss: {loss:.4f}\")\n",
        "\n",
        "# Run the CNN training loop\n",
        "train_cnn()\n"
      ]
    }
  ]
}