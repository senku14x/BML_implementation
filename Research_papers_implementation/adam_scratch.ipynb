{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zdV6nY39we7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f50091a-655a-4338-fdac-3f4217d87e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: x = 4.9000, Loss = 24.0100\n",
            "Iteration 10: x = 3.9101, Loss = 15.2888\n",
            "Iteration 20: x = 2.9699, Loss = 8.8201\n",
            "Iteration 30: x = 2.1244, Loss = 4.5130\n",
            "Iteration 40: x = 1.4112, Loss = 1.9913\n",
            "Iteration 50: x = 0.8524, Loss = 0.7267\n",
            "Iteration 60: x = 0.4512, Loss = 0.2036\n",
            "Iteration 70: x = 0.1917, Loss = 0.0368\n",
            "Iteration 80: x = 0.0455, Loss = 0.0021\n",
            "Iteration 90: x = -0.0210, Loss = 0.0004\n"
          ]
        }
      ],
      "source": [
        "#Adaptive Moment Estimation (Adam's optimizer)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Adam:\n",
        "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "\n",
        "            #params (dict): Dictionary of parameters to optimize. For example: {'W': weight_matrix, 'b': bias_vector}.\n",
        "            #lr (float): Learning rate.\n",
        "            #beta1 (float): Exponential decay rate for the first moment estimates.\n",
        "            #beta2 (float): Exponential decay rate for the second moment estimates.\n",
        "            #epsilon (float): A small constant to prevent division by zero.\n",
        "\n",
        "        self.params = params\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.t = 0  # Time step\n",
        "\n",
        "        # Initialize first (m) and second (v) moment variables for each parameter as zeros.\n",
        "        self.m = {key: np.zeros_like(value) for key, value in params.items()}\n",
        "        self.v = {key: np.zeros_like(value) for key, value in params.items()}\n",
        "\n",
        "    def step(self, grads):\n",
        "\n",
        "        #Performs a single update step using the Adam update rule.\n",
        "\n",
        "        #Parameters:\n",
        "            #grads (dict): Dictionary of gradients corresponding to each parameter.\n",
        "\n",
        "        self.t += 1\n",
        "        for key in self.params.keys():\n",
        "            # Update biased first moment estimate.\n",
        "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
        "            # Update biased second moment estimate.\n",
        "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)\n",
        "            # Compute bias-corrected estimates.\n",
        "            m_hat = self.m[key] / (1 - self.beta1 ** self.t)\n",
        "            v_hat = self.v[key] / (1 - self.beta2 ** self.t)\n",
        "            # Update parameters.\n",
        "            self.params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "# --- Example Usage: Minimizing f(x) = x^2 ---\n",
        "# Our goal is to find the minimum of f(x) = x^2, whose gradient is 2*x.\n",
        "params = {'x': np.array([5.0])}  # Start with x = 5.0\n",
        "optimizer = Adam(params, lr=0.1)  # Set a learning rate of 0.1\n",
        "\n",
        "# Dummy training loop.\n",
        "for i in range(100):\n",
        "    # Compute gradient of f(x) = x^2, which is 2*x.\n",
        "    grads = {'x': 2 * params['x']}\n",
        "\n",
        "    # Update the parameter using Adam.\n",
        "    optimizer.step(grads)\n",
        "\n",
        "    # Compute current loss.\n",
        "    loss = params['x'] ** 2\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Iteration {i}: x = {params['x'][0]:.4f}, Loss = {loss[0]:.4f}\")\n",
        "\n",
        "\n"
      ]
    }
  ]
}