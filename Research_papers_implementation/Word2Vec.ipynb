{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOSelUzJ3B4H",
        "outputId": "3d8b2ed6-c6d9-4c1b-d53c-1d4fc8e5e217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 202651\n",
            "Using 100000 tokens for training.\n",
            "Vocabulary size: 14944\n",
            "enter the model type cbow or skipgramskipgram\n",
            "Total Skip-gram training pairs: 399984\n",
            "\n",
            "Starting training...\n",
            "Epoch 1/10, Average Loss: 7.7010\n",
            "Epoch 2/10, Average Loss: 7.0525\n",
            "Epoch 3/10, Average Loss: 6.8368\n",
            "Epoch 4/10, Average Loss: 6.6935\n",
            "Epoch 5/10, Average Loss: 6.5843\n",
            "Epoch 6/10, Average Loss: 6.4964\n",
            "Epoch 7/10, Average Loss: 6.4237\n",
            "Epoch 8/10, Average Loss: 6.3623\n",
            "Epoch 9/10, Average Loss: 6.3098\n",
            "Epoch 10/10, Average Loss: 6.2644\n",
            "\n",
            "Sample embeddings for a few words:\n",
            "yorkshire,: tensor([-0.2072,  0.6668,  1.4050, -1.2013, -0.3773,  1.1720, -0.8441, -1.7788,\n",
            "        -0.4114,  0.1093,  0.8068,  1.8879, -0.3630, -1.7464, -0.1808, -0.0508,\n",
            "         0.6056, -1.4174, -0.6790,  0.6428, -2.2759,  0.3765, -0.6289, -1.3949,\n",
            "         1.5634, -0.1261, -1.5581,  1.2216, -0.6249,  0.2331, -0.1328,  1.2876,\n",
            "        -0.0466,  0.4289,  0.9631, -0.8077, -0.7650,  0.1858,  1.1256, -1.5491,\n",
            "         0.2572, -0.6824, -0.7180,  0.9227,  1.2410, -1.3316, -0.1917,  0.0312,\n",
            "        -0.3892,  0.7131, -1.4995,  0.8728, -0.4145, -0.0663,  1.1871,  0.6222,\n",
            "        -0.4888,  0.1566,  0.8008,  1.4747, -0.1529,  0.4328, -0.8646,  1.3570,\n",
            "         0.2978, -1.7026, -1.7450,  0.7379, -1.0746, -0.6410,  0.3943, -0.5339,\n",
            "        -0.5013,  1.0410, -0.5272, -0.7647, -1.2634,  1.1041, -0.6930, -1.1558,\n",
            "        -0.9538, -0.9669, -0.2288, -0.1998,  2.1417, -0.6928,  1.3563, -0.7976,\n",
            "         0.9737, -1.7257,  1.6086,  0.8853, -0.1583, -2.8339,  1.4219,  0.0932,\n",
            "         0.9604, -0.5336, -0.7374,  1.8887])\n",
            "proclaim: tensor([-1.0269,  2.3228,  0.8796,  0.4490, -0.3186,  1.1340, -0.5106,  1.0197,\n",
            "         0.2623, -1.1538,  0.2179, -1.0500, -0.6283, -2.0543, -0.4505, -0.1357,\n",
            "        -0.2701,  0.4664, -0.9690,  1.2148, -0.8652, -0.5631,  0.1987,  0.2798,\n",
            "         1.3703, -0.1652, -0.1798,  1.6557,  0.3396,  0.2946, -0.2133, -0.3924,\n",
            "         0.7861,  1.1746, -1.4629, -0.1219,  0.1297, -0.4270, -1.3002,  0.9858,\n",
            "         1.8263, -0.1671,  0.4272,  0.8886, -1.5868,  1.5877, -0.3981, -0.1594,\n",
            "         0.3518, -2.6230, -0.2494,  2.0603, -1.3788,  0.7941,  1.5204, -0.3920,\n",
            "         1.7526,  0.8996, -0.6151,  1.6169,  0.0595, -0.1384, -0.3940,  0.1129,\n",
            "        -0.1325,  0.8982,  0.6158, -0.7862, -0.6851,  0.0930, -0.6108, -1.4066,\n",
            "        -0.0613, -0.7351, -0.2745, -0.9267, -0.2431, -1.5829, -0.7936,  0.3879,\n",
            "         0.2218, -0.6221, -0.4099, -1.5889,  1.3064, -0.9502, -0.3307, -1.0804,\n",
            "        -1.5862, -0.5816, -0.0408, -2.2501, -2.6286, -1.0878, -1.0181, -0.7190,\n",
            "        -0.7160, -0.9865, -1.0328,  0.4765])\n",
            "saint-seducing: tensor([ 0.6761,  0.8782,  0.0738, -0.5651,  0.1378, -0.3294,  0.2911, -1.2979,\n",
            "         0.8835, -1.1712,  0.2397,  0.6533,  1.3383,  0.0291,  1.2878, -1.3490,\n",
            "        -0.5528, -1.4460,  1.0581,  0.4084, -1.0732,  0.2586,  1.9946, -0.0603,\n",
            "         0.1749,  0.3886, -0.2068,  1.1329, -0.7408,  1.0043, -0.4936,  0.1481,\n",
            "        -0.6006,  0.7825,  0.5217,  1.3378, -0.0098, -0.0436,  1.0570, -0.7487,\n",
            "         0.2751, -0.9621,  0.9389,  0.8075, -1.6401, -0.4299, -1.3112, -0.9887,\n",
            "        -0.9858, -0.7579, -0.2330, -0.2151,  0.2544,  0.2125, -1.7114, -0.5536,\n",
            "        -2.2972,  0.3951,  1.0611,  0.0386,  0.9200, -0.3348, -0.7169,  0.8052,\n",
            "        -0.6675,  0.3824,  1.5246,  0.4979, -1.8445, -1.4507,  0.2273, -2.0660,\n",
            "         1.0014, -0.9876, -0.8731, -0.4490, -0.5999, -0.6383, -0.6610,  0.7595,\n",
            "         1.0685, -1.9361, -0.5938, -1.0539,  0.8816,  1.8844,  0.7033,  0.1093,\n",
            "         0.9657,  0.1833,  0.0919, -0.4966, -1.4037, -0.6417,  1.5022,  0.9101,\n",
            "        -1.2467,  0.2488,  0.4844,  0.6983])\n",
            "joy;: tensor([ 0.5777, -0.5799, -1.4366,  0.0374,  1.2589,  0.9787,  0.5980, -0.4858,\n",
            "        -0.7820,  0.4506, -0.3784, -0.2971, -0.1004,  0.6956,  0.3805,  0.5951,\n",
            "        -0.1777, -2.0734,  1.0937, -0.1358, -0.7304, -0.2806,  0.1977, -0.4196,\n",
            "        -0.0730, -0.0104, -0.0672, -0.8053,  0.0058, -0.4427, -2.1433, -1.5334,\n",
            "        -0.6947,  0.5362,  2.0761,  1.4004,  1.4250,  0.1886,  0.2225,  0.5913,\n",
            "         1.4444,  1.8504,  0.2929, -1.9384,  0.4420,  1.7280,  0.9975, -1.3976,\n",
            "        -0.8796,  1.3241, -0.3332, -0.2668,  0.6803,  0.5479,  0.7449, -0.2735,\n",
            "         0.7209,  0.1559, -0.7833, -0.1324,  0.1102, -2.0004,  0.1679,  0.8946,\n",
            "        -0.4581, -0.5073, -0.5913,  1.3813, -1.5529, -1.9302, -1.8570, -1.2362,\n",
            "         1.1353,  0.6224, -0.7062, -0.1987, -2.0456,  0.3538,  0.2050, -1.2043,\n",
            "         1.6228,  0.8022,  0.1929,  1.6730,  0.1829, -1.1451,  0.0319,  0.6478,\n",
            "         0.4665,  0.9994, -0.0043,  1.1744,  0.8040,  0.5319, -0.2947, -1.0167,\n",
            "        -0.4672, -0.2671, -0.6246, -1.3502])\n",
            "folly,: tensor([-0.1210,  0.0137, -0.0833, -0.6230, -0.1639,  0.0196,  0.2280, -1.1136,\n",
            "        -0.2013,  1.8713, -0.1530,  0.8829,  0.9591, -0.2966, -1.0595,  1.1616,\n",
            "         0.6690, -0.9984, -0.5364,  0.1858,  0.4546, -0.3959, -1.2556,  0.7395,\n",
            "         1.2905,  0.9155,  0.7650, -2.3911, -0.1627, -0.1211, -0.4004, -1.3856,\n",
            "        -2.9381, -0.9843,  0.4595,  0.4159,  0.1578, -0.7326, -1.8006, -0.8480,\n",
            "        -0.7852, -0.4418,  0.1408,  0.7020,  0.5501,  1.7479,  0.5996,  0.0869,\n",
            "         0.0074,  0.0417, -0.1451, -0.0929, -0.2592, -1.8932,  1.6818, -1.0129,\n",
            "         0.5351,  1.3846, -0.5047,  0.6356,  1.2820,  0.7385,  0.7784,  1.3976,\n",
            "         1.5624, -2.1060,  0.2375, -0.7595,  0.6097, -1.1797, -0.1117, -1.0093,\n",
            "         0.8084, -0.2595,  0.6262, -0.1617,  0.9867,  0.6797, -0.1701,  0.8028,\n",
            "         0.0163, -0.5329,  0.8330,  0.0116,  0.1997, -0.6076, -0.1395, -0.6248,\n",
            "        -0.9095, -0.1518, -0.9536,  0.5298,  0.9363, -0.1528,  0.5916,  1.1230,\n",
            "         2.3675, -0.3385,  0.4270,  1.2240])\n"
          ]
        }
      ],
      "source": [
        "# Downloading the Tiny Shakespeare dataset\n",
        "!wget -q \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\" -O tiny_shakespeare.txt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Reading and preprocessing the dataset\n",
        "with open(\"tiny_shakespeare.txt\", \"r\") as f:\n",
        "    text = f.read().lower()\n",
        "\n",
        "# Tokenizing\n",
        "words = text.split()\n",
        "print(f\"Total tokens: {len(words)}\")\n",
        "\n",
        "#  taking 100k tokens only\n",
        "words = words[:100000]\n",
        "print(f\"Using {len(words)} tokens for training.\")\n",
        "\n",
        "# Building the vocabulary\n",
        "vocab = set(words)\n",
        "word2idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx2word = {i: word for word, i in word2idx.items()}\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "model_type = input(\"enter the model type cbow or skipgram\")\n",
        "\n",
        "window_size = 2\n",
        "\n",
        "def generate_cbow_data(words, word2idx, window_size):\n",
        "    data = []\n",
        "    for i in range(window_size, len(words) - window_size):\n",
        "        context = []\n",
        "        for j in range(i-window_size,i+window_size+1):\n",
        "          if j!=i:\n",
        "            context.append(word2idx[words[j]])\n",
        "        target = word2idx[words[i]]\n",
        "        data.append((context, target))\n",
        "    return data\n",
        "\n",
        "def generate_skipgram_data(words, word2idx, window_size):\n",
        "    data = []\n",
        "    for i in range(window_size, len(words) - window_size):\n",
        "      target = word2idx[words[i]]\n",
        "      for j in range(i-window_size,i+window_size+1):\n",
        "        if j!=i:\n",
        "          context = word2idx[words[j]]\n",
        "          data.append((context, target))\n",
        "    return data\n",
        "\n",
        "#generating the data according to the model selected\n",
        "\n",
        "if model_type == 'cbow':\n",
        "    training_data = generate_cbow_data(words, word2idx, window_size)\n",
        "    print(f\"Total CBOW training pairs: {len(training_data)}\")\n",
        "elif model_type == 'skipgram':\n",
        "    training_data = generate_skipgram_data(words, word2idx, window_size)\n",
        "    print(f\"Total Skip-gram training pairs: {len(training_data)}\")\n",
        "else:\n",
        "    raise ValueError(\"Invalid model_type. Choose 'cbow' or 'skipgram'.\")\n",
        "\n",
        "# Defining the CBOW model\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context_words):\n",
        "        embeds = self.embeddings(context_words)  # [batch_size, context_size, embedding_dim]\n",
        "        avg_embeds = torch.mean(embeds, dim=1)     # Average over context words: [batch_size, embedding_dim]\n",
        "        out = self.linear(avg_embeds)             # [batch_size, vocab_size]\n",
        "        return out\n",
        "\n",
        "# Defining The SkipGram model\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, target_word):\n",
        "        embeds = self.embeddings(target_word)\n",
        "        out = self.linear(embeds)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 100\n",
        "\n",
        "if model_type == 'cbow':\n",
        "    model = CBOW(vocab_size, embedding_dim).to(device)\n",
        "elif model_type == 'skipgram':\n",
        "    model = SkipGram(vocab_size, embedding_dim).to(device)\n",
        "\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    if model_type == 'cbow':\n",
        "        # Each training example: (context, target)\n",
        "        for context, target in training_data:\n",
        "            # Converting to tensors and moving to device\n",
        "            context_tensor = torch.tensor([context], dtype=torch.long).to(device)  # [1, context_size]\n",
        "            target_tensor = torch.tensor([target], dtype=torch.long).to(device)    # [1]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(context_tensor)  # [1, vocab_size]\n",
        "            loss = loss_function(logits, target_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "    elif model_type == 'skipgram':\n",
        "        for target, context in training_data:\n",
        "            target_tensor = torch.tensor([target], dtype=torch.long).to(device)\n",
        "            context_tensor = torch.tensor([context], dtype=torch.long).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(target_tensor)  # [1, vocab_size]\n",
        "            loss = loss_function(logits, context_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(training_data)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# After training,sample embeddings for a few words\n",
        "embeddings = model.embeddings.weight.data.cpu()\n",
        "print(\"\\nSample embeddings for a few words:\")\n",
        "for word in list(word2idx.keys())[:5]:\n",
        "    print(f\"{word}: {embeddings[word2idx[word]]}\")"
      ]
    }
  ]
}